/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_dataset.py:261: DtypeWarning: Columns (18,19,20,21,22) have mixed types. Specify dtype option on import or set low_memory=False.
  self.txt_path = pd.read_csv(txt_path)
/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_dataset.py:307: DtypeWarning: Columns (17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.
  self.txt_path = pd.read_csv(txt_path)
/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
directory "../checkpoints/" existing for save checkpoint!
#########################################
Be patient..., checking checkpoint now...
Start training from 0 epoch
#########################################
training start!
/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_trainer.py:91: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|                                                                                                                                                                                                                                                                                                                                            | 0/21 [00:00<?, ?it/s]/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_trainer.py:117: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():                                                                                                                                                                                                                                                                                                                             | 0/1406 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                                                                          | 0/1406 [00:02<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                                                                            | 0/21 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/data1/1shared/lijun/ecg/E-Zero/pretrain/main.py", line 106, in <module>
    ddp_main()
  File "/data1/1shared/lijun/ecg/E-Zero/pretrain/main.py", line 103, in ddp_main
    trainer.train_w_TextEmb(train_dataset, val_dataset, config['zeroshot'])
  File "/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_trainer.py", line 125, in train_w_TextEmb
    output_dict = self.model(ecg, input_ids, attention_mask)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_builder.py", line 326, in forward
    text_emb = self.get_text_emb(input_ids, attention_mask)
  File "/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_builder.py", line 294, in get_text_emb
    outputs = self.lm_model(input_ids=input_ids,
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1130, in forward
    outputs = block(
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 626, in forward
    hidden_states = attn_output + residual
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 39.12 MiB is free. Process 2590489 has 384.00 MiB memory in use. Process 2590487 has 384.00 MiB memory in use. Process 2590488 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 22.47 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 70.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data1/1shared/lijun/ecg/E-Zero/pretrain/main.py", line 106, in <module>
[rank0]:     ddp_main()
[rank0]:   File "/data1/1shared/lijun/ecg/E-Zero/pretrain/main.py", line 103, in ddp_main
[rank0]:     trainer.train_w_TextEmb(train_dataset, val_dataset, config['zeroshot'])
[rank0]:   File "/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_trainer.py", line 125, in train_w_TextEmb
[rank0]:     output_dict = self.model(ecg, input_ids, attention_mask)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_builder.py", line 326, in forward
[rank0]:     text_emb = self.get_text_emb(input_ids, attention_mask)
[rank0]:   File "/data1/1shared/lijun/ecg/E-Zero/pretrain/../utils/utils_builder.py", line 294, in get_text_emb
[rank0]:     outputs = self.lm_model(input_ids=input_ids,
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1130, in forward
[rank0]:     outputs = block(
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/lijun/anaconda3/envs/ecg/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 626, in forward
[rank0]:     hidden_states = attn_output + residual
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 39.12 MiB is free. Process 2590489 has 384.00 MiB memory in use. Process 2590487 has 384.00 MiB memory in use. Process 2590488 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 22.47 GiB memory in use. Of the allocated memory 21.82 GiB is allocated by PyTorch, and 70.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
